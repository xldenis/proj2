\documentclass[10pt,twocolumn]{article}
\usepackage{amsthm, amssymb, geometry, mathrsfs}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\begin{document}
\section*{Abstract}

\section*{Introduction}

Categorization of articles in scientific journals is a useful step towards making it easy for future researchers to find and build upon the body of knowledge in their field.  Normally, it is fairly obvious to a human editor how to categorize a new article, so an automated system to classify documents is unnecessary.  However, with the rise of open access journals, more and more articles are being indexed into journals other than the ones they were published in, which might use a different categorization scheme.  When a one-to-one mapping of categories is impossible, it can be time-consuming to review large numbers of articles and find a suitable category from within the new categorization.

\section*{Related Work}

The problem of automatic text classification using machine learning has been extensively studied on several datasets, including the 20 Newsgroups dataset and the Reuters dataset.  The 20 Newsgroups dataset included a collection of almost 20,000 newsgroup threads from twenty different categories.  Some of the categories are closely related, such as ``rec.sport.baseball'' and ``rec.sport.hockey'', and some categories have nothing to do with the others, such as ``misc.forsale''.


Lang, Ken.  "Newsweeder: Learning to filter netnews".  Proceedings of the Twelfth International Conference on Machine Learning", p. 331-339, 1995.

The Reuters-21578 dataset provides a similar number of news articles with corresponding categories for machine learning research.  Released in 1990, it provides a different type of text, given that the 20 Newsgroups are dicussions from internet users and the Reuters articles are news broadcasts not meant for interaction.  The Reuters-21578 dataset has been superceded by the RCV1 dataset, which contains over 800,000 labeled news articles.

Reuters Corpus, Volume 1, English language, 1996-08-20 to 1997-08-19
(Release date 2000-11-03, Format version 1, correction level 0)

Philip J. Hayes and Steven P. Weinstein. "CONSTRUE/TIS: A System for Content-Based Indexing of a Database of News Stories." Second Annual Conference on Innovative Applications of Artificial Intelligence, 1990. 

The dataset under consideration here is different in that the full text of the articles are not available; only a part of the complete data is available, the abstract.

\begin {enumerate}
\item 20newgroups
\item reuters
\item all mnb papers
\item knn papers
\item tfidf papers
\item svd papers
\end {enumerate}

\section*{Data Pre-processing}

Simple transformations were performed on the raw data to normalize it and extract the most meaning. A further stage of transformations was performed at the corpus level on the ensemble of extracted features.

\subsection*{Latex Removal}

Mathematical equations in latex generally appear as a mathematical expression between two \$ symbols. The primary issue with this latex formatting is that semantically similar phrases can take very different and hard to parse appearances. To remove this kind of formatting, all text between two matched \$ symbols was extracted. All instances of mathematical expressions are replaced by a symbol: $math123$. In this way, mathematical expressions are simply counted, and this count used as a feature, rather than each component of the expression parsed and used as individual features.  This produces features that are more easily comparable across documents and categories.

\subsection*{Text Normalization}

After removing all latex formatting, the text is normalized using standard techniques[cite]. First the entire document is transformed to lowercase, reducing vocabulary size significantly. Next, it is broken up into word chunks by spaces, removing all punctuation in the process. Theses two approaches can reduce the size of the vocabulary significantly. A Porter stemmer [cite] was employed to reduce words to their root.  For example, after stemming, the words ``quantum'' and ``quanta'' are considered equivalent, and counted together. The Porter stemmer groups different verb tenses, adjectives and nouns that have identical semantic meaning, and plurals of words.  The main advantage of employing the Porter stemmer is the reduction of sparsity of the features by grouping features into semantic groups.  It was found that using the stemmer reduced the size of the corpus vocabulary by approximately $25\%$.  To further reduce the size of the vocabulary, words of length less than 4 characters were discarded, which reduced our feature space by approximately $20\%$.  As the final step to pre-processing the abstract data, a list of English stop words, was filtered out of our feature matrices.  These stop words represent the most common words in the English language, and are assumed not to have any information about the contents of the document.  For example, words like ``a'', ``then'', and ``but'' cannot be expected to contain information about what type of document they appear in, but may appear to correlate to one of the classes in the training set and result in overfitting.

\section*{Feature Design and Selection}

After raw data was pre-processed using the above steps, two sets of features were considered: word count, and TF-IDF.

\subsection*{Word Count}

This type of feature is simple, consisting of the word counts $n_{wd}$ in each document for all words. From this set of features, the instances of words across an entire class can easily be computed, and used in Naive Bayes classification to estimate $P(w | c)$ for every word $w$ and class $c$. The set of all words, even after stemming and filtering, was too large to be used in the scikit-learn classifiers, so several strategies were employed to select the best word counts.  First, the 2000 most frequently appearing words were selected, and found to give fairly good performance with the scikit-learn random forest classifier.  A qualitative assessment of these words assessed that many of the most common words contained no semantic information, and that the optimal feature set might lie with less frequent words that appeared in only one of the classes, so an arbitrary number of most frequent features was skipped, and the next 2000 features used.  This was found to decrease the quality of prediction, but an exhaustive search of the feature space was not performed due to the promise of TF-IDF features.


http://www.csie.ntu.edu.tw/~r95038/Try/paper/feature%20selection%20on%20TFIDF.pdf


Perf

\subsection*{TF-IDF}

Term Frequency-Inverse Document Frequency calculates $n'_{wd} = \log (n_{wd} + 1) \times log (|D| / df(w))$ where $df(\cdot)$ is the \emph{document frequency} of a word, or the number of documents containing a word. The purpose of this transformation is to compensate for the influence of common words and emphasize the influence of rare ones in word counts. While TF-IDF can compensate for rare and frequent words it does not consider document length. Document length is normalized by dividing the feature vector by the average document length, according to the $L_2$ Norm.

Perf

A second transformation that was explored is a class-length prior distribution for word frequencies. Word frequencies for each class were scaled by the total word count within the class. The purpose of this rescaling is to consider the words in a manner purely relative to their own class, rather than in comparison to other classes.

Perf

When classifying text with kNN, the dimensionality of the vector space is problematic. Although the distance metric employed accomodates higher dimensions more easily, when faced with the dimensions of the word space, some form of feature selection is required.

\subsection*{SVD}

Singular value decomposition is a technique that will break a matrix $M$ into three matrices $U, S, V^T$ such that $U \cdot S \cdot V^T = M$. The use of SVD in feature selection is that reducing the rank of the diagonal matrix $S$ to $k$, it will produce the closest matrix $M'$ of degree $k$ to $M$. By interpreting the feature vectors as a document-feature matrix, or in this case as a tf-idf matrix, we can find the best features. 

The main obstacle encountered using SVD is that allowable python packages for this project do not have efficient implementations. Calculating the SVD for a matrix of 96,000 rows by approximately 88,000 columns took approximately 1 hour and required several gigabytes of RAM to store the three output matrices. Additionally, since the kNN classification suffered from slow performance, adding matrix multiplication to each distance calculation would further reduce throughput.

Additionally, there is no theoretical evidence that using the features with highest values in the $S$ matrix will be the best features in classification.

\subsection*{Random}

In research done for this work, a conclusive strategy for selecting a subset of features was not determined, so selecting a random subset was attempted as a baseline.  Selecting the optimal $k$ features could be accomplished using cross-validation or gradient descent, but not implemented in this work.

\subsection*{Most Frequent}

Another strategy for selecting the optimal set of features could be to pick the $k$ most frequent words. This approach has several obvious problems. First, and most obvious is the issue of stop words, words which are frequent but carry little meaning. While TF-IDF will give a low score to these words, choosing the $k$ most frequent words will ignore these scores. Second, this method of selection assumes that the most frequent words contain the most information about document class. It ignores situations where two seperate terms may by highly correlated with half of each class but not in the $k$ most frequent words. These problems can be mitigated by filtering out stop words ahead of time and experimenting with values of $k$ but will never completely disappear.

\section*{Algorithm Selection}

The types of algorithm usable for text classification are limited by the large feature space. Several types of algorithms have shown good performance in the literature, including: LLSF, Multinomial Naive Bayes, Decision Trees, and kNN. A multinomial Naive Bayesian classifier was implemented to measure baseline performance for the different feature selection strategies. Naive Bayes is known for high performance in text envirnoments since its structure allows for efficient learning of large numbers of features.

\subsection*{Baseline: Naive Bayes using word counts}

In order to establish a baseline for future algorithms, the simplest version of Naive Bayes was implemented. Naive Bayes estimates the probability $P(c|w)$ through the use of Bayes Rule.

\[ \frac {P(c)P(w|c)} {P(w)} \]

While $P(c)$ can be estimated easily by counting the raw occurences of the label $c$ within the training data, $P(w|c)$ requires more work. $P(w|c)$ is estimated by the following formula:

\[\frac {\sum_{d \in D_c} n_{wd}} {\sum_{w'} 2\sum_{d\in D_c} n_{wd}} \]

While performing sums is efficient for a computer, calculating the denominator on the entire training set can prohibitive with $O(mn)$ for $m$ features and $n$ documents. To solve this issue, the sums are reversed, taking advantage of the sparsity of features within documents. While the same order-bound is preserved, given that a single abstract will not contain more than a few hundred unique words, less iterations are expected.

\subsection*{Standard: (Naive Bayes with TF-IDF \& Ensemble) and kNN with TF-IDF}

For the `standard' approach, the use of TF-IDF features was explored with multinomial naive bayes and k-Nearest Neighbours classifiers. Another approach considered was Text Weighted Complement Naive Bayes.

For Naive Bayes, rewriting the algorithm to operate on a two dimensional document-feature matrix rather than the actual class based word counts allows changing the features. By doing so, raw word counts can be replaced with TF-IDF values. 

kNN attempts to search for the $k$ nearest points to the query point. The first step then, is to define some metric (distance) function. For this, the cosine similarity of TF-IDF vectors was used,

\[
sim(A,B) = \frac {\sum_{w \in A \cap B} A_w \times B_w} {\| A \| \| B \|}
\]
Where $A,B$ are TF-IDF vectors. Once the metric function is defined, kNN simply becomes a matter of iterating over the training set to find the $k$ documents with the highest scores. To label a test document, the most frequent of the $k$ nearest labels is chosen.

The primary difficulty with kNN is that each query will be computationally expensive, because each test sample must be compared to every sample in the training set. Due to this requirement, the trick used for efficiency in Naive Bayes couldn't be used, requiring an inner loop of 80,000 iterations if all the training data is used. This issue is why feature selection and reduction is paramount in kNN, since it will significantly reduce computation time.

\#\#\# Optimization of kNN

\subsection*{Advanced: Random Forests}

Random forests have produced excellent results in many classification contexts. The scikit-learn random forest classifier, using the 2000 most frequent word counts as features, classified the test set with $82\%$ accuracy.

\section*{Parameter Selection}

The classifiers used in this study have few parameters to select, and the most important factor that affected classification was the type of features selected.  Increasing the number of features used with Naive Bayes and the Random Forest classifiers improved performance, and were only not maximized due to time and technical considerations.

For the kNN classifier, the number of closest training samples, $k$, could be tuned through cross-validation, but due to the time required to classify even one document, a full k-fold validation of this parameter was not performed.

\subsection*{Normalization Length}

\subsection*{Vocabulary size}

\subsection*{Testing and Validation}

\subsection*{k-fold X-Val}

ROC

Confusion
 
\section*{Discussion}

\subsection*{kNN performance}

\subsection*{Difficulty of TF-IDF}

\section*{Future Work}

\end {document}
